{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761359dd",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Recsys2022 Challenge \n",
    "\n",
    "NVIDIA-Merlin team participated in [Recsys2022 challenge](http://www.recsyschallenge.com/2022/index.html) and secured 3rd position. This notebook contains the various techniques used in the solution.\n",
    "\n",
    "### Learning Objective\n",
    "In this notebook, we will learn the importance of the concepts that improved the results of the competition significantly.\n",
    "\n",
    "- ##### Label smoothing\n",
    "     When the probabilities predicted by a Classification model are higher than its accuracy we say the model is overconfident. It can be prevented by using Label smoothing. This technique basically, transforms One-hot encoded labels into smoothed labels. \n",
    "$$  \\begin{array}{l}\n",
    "y_{l} \\ =\\ ( 1\\ -\\ \\alpha \\ ) \\ *\\ y_{o} \\ +\\ ( \\alpha \\ /\\ L)\\\\\n",
    "\\alpha :\\ Label\\ smoothing\\\\\n",
    "L:\\ Total\\ number\\ of\\ label\\ classes\\\\\n",
    "y_{o} :\\ One-hot\\ encoded\\ label\\ vector\n",
    "\\end{array}\n",
    "$$\n",
    "When α is 0, we have the original one-hot encoded labels, and as α increases, we move towards smoothed labels. Read [this](https://arxiv.org/abs/1906.02629) paper to learn more about it.\n",
    "\n",
    "\n",
    "- ##### Temperature Scaling\n",
    "    Similar to Label Smoothing, Temperature Scaling is done to reduce the overconfidence of a model. In this, we divide the logits (inputs to the softmax function) by a scalar parameter (T) . For more information on Temperature Scaling read [this](https://arxiv.org/pdf/1706.04599.pdf) paper.\n",
    "$$ softmax\\ =\\ \\frac{e\\ ^{( z_{i} \\ /\\ \\ T)}}{\\sum _{j} \\ e^{( z_{j} \\ /\\ T)} \\ } $$\n",
    "\n",
    "\n",
    "- ##### Weight Tying\n",
    "In this technique, we share the Embedding layer's weights which is used to convert the input to embeddings, as the softmax weights,  to convert hidden layer output to softmax layer output. This drastically reduces the number of parameters and allows the model to train better. For more information read [this](https://arxiv.org/pdf/1608.05859v3.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d21e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import pandas as pd \n",
    "\n",
    "import nvtabular as nvt\n",
    "from merlin.dag import ColumnSelector\n",
    "from merlin.io import Dataset\n",
    "from merlin.schema import Schema, Tags\n",
    "from nvtabular.ops import (\n",
    "    AddMetadata,\n",
    ")\n",
    "from merlin.schema.tags import Tags\n",
    "from utils import get_drepessi_recsys2022_dataset\n",
    "\n",
    "DATA_FOLDER = 'dressipi'\n",
    "OUTPUT_FOLDER = 'dressipi_processed'\n",
    "DATETIME_CONVERTION = 'ms'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3795b5e",
   "metadata": {},
   "source": [
    "## Dressipi\n",
    "The [Dressipi](http://www.recsyschallenge.com/2022/dataset.html) dataset contains 1.1 M online retail sessions that resulted in a purchase. It provides details about items that were viewed in a session, the item purchased at the end of the session and numerous features of those items. The task of this competition was, given a sequence of items predict which item will be purchased at the end of a session.\n",
    "\n",
    "\n",
    "<img src=\"images/dressipi.jpeg\" alt=\"dressipi_dataset\" style=\"width: 400px; float: center;\">  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e1cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, item_features, sessions = get_drepessi_recsys2022_dataset(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f0fe87",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401cdf8e",
   "metadata": {},
   "source": [
    "### Categorify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ff199e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1292: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "item_features_names = item_features.columns[1:].tolist()\n",
    "cat_features = ['session_id', 'item_id', 'purchase_id'] + item_features_names >> nvt.ops.Categorify()\n",
    "all_data = Dataset(sessions).to_ddf()\n",
    "\n",
    "features = ['timestamp','date'] + cat_features\n",
    "dataset = Dataset(all_data)\n",
    "workflow0 = nvt.Workflow(features)\n",
    "workflow0.fit(dataset)\n",
    "\n",
    "# transform data\n",
    "train_0 = workflow0.transform(Dataset(train))\n",
    "valid_0 = workflow0.transform(Dataset(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77871662",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52fdef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 1.56 s, total: 3 s\n",
      "Wall time: 4.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = train_0.head().columns.tolist()\n",
    "\n",
    "# Define Groupby Operator\n",
    "to_aggregate = {\n",
    "    'date': [\"first\", \"last\"],\n",
    "    'item_id': [\"list\"],\n",
    "    'timestamp': [\"list\"],\n",
    "    'purchase_id': ['first'],\n",
    "}\n",
    "for name in item_features_names: \n",
    "    to_aggregate[name] = ['list']\n",
    "    \n",
    "groupby_features = features >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"date\"],\n",
    "    aggs= to_aggregate,\n",
    "    name_sep=\"_\")\n",
    "\n",
    "# Add tags needed for the t4rec models definition\n",
    "item_list = groupby_features['item_id_list'] >> nvt.ops.AddMetadata(tags=[Tags.SEQUENCE, Tags.ITEM, Tags.ITEM_ID, Tags.LIST])\n",
    "feature_list = groupby_features[[name+'_list' for name in item_features_names]]>> nvt.ops.AddMetadata(tags=[Tags.SEQUENCE, Tags.ITEM, Tags.LIST])\n",
    "other_features = groupby_features['session_id', 'date_first', 'date_last','timestamp_list']\n",
    "target_feature = groupby_features['purchase_id_first'] >> nvt.ops.AddMetadata(tags=[Tags.TARGET])\n",
    "\n",
    "workflow1 = nvt.Workflow(item_list + feature_list + other_features + target_feature)\n",
    "workflow1.fit(train_0)\n",
    "\n",
    "# transform data\n",
    "train_1 = workflow1.transform(train_0)\n",
    "valid_1 = workflow1.transform(valid_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470599bc",
   "metadata": {},
   "source": [
    "### Truncate and Padding for a Maximum Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f9de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSIONS_MAX_LENGTH = 20\n",
    "list_cols = [col for col in train_1.head().columns if 'list' in col and 'date' not in col]\n",
    "truncated_features = list_cols >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH, pad=True) >> nvt.ops.Rename(postfix = '_seq')\n",
    "\n",
    "final_features = [\n",
    "    'session_id', 'date_first', 'date_last', 'item_id_list', 'purchase_id_first'\n",
    "]\n",
    "\n",
    "workflow2 = nvt.Workflow(final_features + truncated_features)\n",
    "workflow2.fit(train_1)\n",
    "\n",
    "# transform data\n",
    "train_2 = workflow2.transform(train_1)\n",
    "valid_2 = workflow2.transform(valid_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257bc17b",
   "metadata": {},
   "source": [
    "### Save processed data to Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04920455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.3 s, sys: 22.6 s, total: 42.9 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds = Dataset(train_2.to_ddf().sort_values('date_last'), schema=train_2.schema)\n",
    "valid_ds = Dataset(valid_2.to_ddf().sort_values('date_last'), schema=valid_2.schema)\n",
    "\n",
    "train_ds.to_parquet(os.path.join(OUTPUT_FOLDER, \"train/\"), output_files=10)\n",
    "valid_ds.to_parquet(os.path.join(OUTPUT_FOLDER, \"valid/\"), output_files=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd02326",
   "metadata": {},
   "source": [
    "## Training - MLP\n",
    "\n",
    "A sequential-MLP model with average of the sequence as final representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79815754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from merlin.io import Dataset\n",
    "from merlin.schema import Tags\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "from merlin.models.tf.dataset import BatchedDataset\n",
    "from merlin.models.tf.utils.tf_utils import extract_topk\n",
    "import numpy as np\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf import InputBlock\n",
    "from merlin.models.tf.models.base import Model\n",
    "from merlin.models.tf.core.aggregation import SequenceAggregation, SequenceAggregator\n",
    "from merlin.models.tf.core.transformations import (\n",
    "    ItemsPredictionWeightTying,\n",
    "    L2Norm,\n",
    "    LogitsTemperatureScaler,\n",
    ")\n",
    "\n",
    "DATA_FOLDER = 'dressipi'\n",
    "DATA_PROCESSED_FOLDER = 'dressipi_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6f8a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1292: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(DATA_PROCESSED_FOLDER, 'train/*.parquet'), shuffle=False)\n",
    "valid = Dataset(os.path.join(DATA_PROCESSED_FOLDER, 'valid/*.parquet'), shuffle=False)\n",
    "\n",
    "purchases = pd.read_csv(os.path.join(DATA_FOLDER, \"train_purchases.csv\"))\n",
    "item_map = pd.read_parquet(\n",
    "    os.path.join(\"categories\", \"unique.item_id.parquet\"))['item_id'].to_dict()\n",
    "session_map = pd.read_parquet(\n",
    "    os.path.join(\"categories\", \"unique.session_id.parquet\"))['session_id'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88abfb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item_id_list_seq</td>\n",
       "      <td>(Tags.LIST, Tags.SEQUENCE, Tags.CATEGORICAL, T...</td>\n",
       "      <td>int64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.item_id.parquet</td>\n",
       "      <td>23497.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23497</td>\n",
       "      <td>item_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>purchase_id_first</td>\n",
       "      <td>(Tags.TARGET, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.purchase_id.parquet</td>\n",
       "      <td>18908.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18908</td>\n",
       "      <td>purchase_id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'item_id_list_seq', 'tags': {<Tags.LIST: 'list'>, <Tags.SEQUENCE: 'sequence'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.ITEM_ID: 'item_id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.item_id.parquet', 'embedding_sizes': {'cardinality': 23497.0, 'dimension': 449.0}, 'domain': {'min': 0, 'max': 23497, 'name': 'item_id'}}, 'dtype': dtype('int64'), 'is_list': True, 'is_ragged': False}, {'name': 'purchase_id_first', 'tags': {<Tags.TARGET: 'target'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'start_index': 0.0, 'cat_path': './/categories/unique.purchase_id.parquet', 'embedding_sizes': {'cardinality': 18908.0, 'dimension': 397.0}, 'domain': {'min': 0, 'max': 18908, 'name': 'purchase_id'}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_model = train.schema.select_by_name(['item_id_list_seq', 'purchase_id_first'])\n",
    "schema_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26083f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d55bbb",
   "metadata": {},
   "source": [
    "### Model\n",
    "InputBlock which takes sequential features, concatenate them and return the sequence of interaction embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1b13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_block = InputBlock(\n",
    "        schema_model,\n",
    "        aggregation='concat',\n",
    "        seq=True,\n",
    "        max_seq_length=20,\n",
    "        embedding_options=mm.EmbeddingOptions(embedding_dim_default=128),\n",
    "        split_sparse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b913b4",
   "metadata": {},
   "source": [
    "MLPBlock to get the sequence of hidden representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886d1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "                [64, 128],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "                dropout=0.01,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d700c022",
   "metadata": {},
   "source": [
    "Multi-Classiffication Prediction head which has\n",
    "- Layer Normalization\n",
    "- Weight Tying\n",
    "- Labels as One-hot encoded vectors, used for label smoothing \n",
    "- Temperature Scaling to reduce the overconfidence of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfe27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_call = L2Norm().connect(\n",
    "    ItemsPredictionWeightTying(schema_model), \n",
    "    mm.LabelToOneHot(), \n",
    "    LogitsTemperatureScaler(temperature=2)\n",
    ")\n",
    "\n",
    "prediction_task = mm.MultiClassClassificationTask(\n",
    "    target_name=\"purchase_id_first\",\n",
    "    pre=prediction_call,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd629ec",
   "metadata": {},
   "source": [
    "Now, we connect all the blocks togther to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "464cb20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 18:02:40.983457: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "model_mlp = Model(input_block, mlp_block, SequenceAggregator(SequenceAggregation.MEAN), prediction_task)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=3e-1,\n",
    "    clipnorm=True\n",
    ")\n",
    "\n",
    "# model_mlp.compile(optimizer=optimizer, run_eagerly=False)\n",
    "model_mlp.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=True,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.2),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179f5e2",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe70956b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1799/1799 [==============================] - 477s 257ms/step - loss: 8.2775 - recall_at_100: 0.2593 - mrr_at_100: 0.0511 - ndcg_at_100: 0.0896 - map_at_100: 0.0511 - precision_at_100: 0.0026 - regularization_loss: 0.0000e+00 - val_loss: 8.2564 - val_recall_at_100: 0.3943 - val_mrr_at_100: 0.0966 - val_ndcg_at_100: 0.1530 - val_map_at_100: 0.0966 - val_precision_at_100: 0.0039 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1799/1799 [==============================] - 461s 253ms/step - loss: 7.8452 - recall_at_100: 0.3870 - mrr_at_100: 0.0984 - ndcg_at_100: 0.1536 - map_at_100: 0.0984 - precision_at_100: 0.0039 - regularization_loss: 0.0000e+00 - val_loss: 8.2082 - val_recall_at_100: 0.4103 - val_mrr_at_100: 0.1063 - val_ndcg_at_100: 0.1646 - val_map_at_100: 0.1063 - val_precision_at_100: 0.0041 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1799/1799 [==============================] - 455s 250ms/step - loss: 7.6391 - recall_at_100: 0.4524 - mrr_at_100: 0.1235 - ndcg_at_100: 0.1869 - map_at_100: 0.1235 - precision_at_100: 0.0045 - regularization_loss: 0.0000e+00 - val_loss: 8.1962 - val_recall_at_100: 0.4225 - val_mrr_at_100: 0.1089 - val_ndcg_at_100: 0.1689 - val_map_at_100: 0.1089 - val_precision_at_100: 0.0042 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1799/1799 [==============================] - 455s 250ms/step - loss: 7.5130 - recall_at_100: 0.4909 - mrr_at_100: 0.1394 - ndcg_at_100: 0.2073 - map_at_100: 0.1394 - precision_at_100: 0.0049 - regularization_loss: 0.0000e+00 - val_loss: 8.1966 - val_recall_at_100: 0.4290 - val_mrr_at_100: 0.1157 - val_ndcg_at_100: 0.1756 - val_map_at_100: 0.1157 - val_precision_at_100: 0.0043 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1799/1799 [==============================] - 464s 254ms/step - loss: 7.2950 - recall_at_100: 0.5572 - mrr_at_100: 0.1661 - ndcg_at_100: 0.2421 - map_at_100: 0.1661 - precision_at_100: 0.0056 - regularization_loss: 0.0000e+00 - val_loss: 8.2619 - val_recall_at_100: 0.4284 - val_mrr_at_100: 0.1186 - val_ndcg_at_100: 0.1783 - val_map_at_100: 0.1186 - val_precision_at_100: 0.0043 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1799/1799 [==============================] - 458s 251ms/step - loss: 7.2467 - recall_at_100: 0.5725 - mrr_at_100: 0.1728 - ndcg_at_100: 0.2507 - map_at_100: 0.1728 - precision_at_100: 0.0057 - regularization_loss: 0.0000e+00 - val_loss: 8.2523 - val_recall_at_100: 0.4325 - val_mrr_at_100: 0.1223 - val_ndcg_at_100: 0.1821 - val_map_at_100: 0.1223 - val_precision_at_100: 0.0043 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1799/1799 [==============================] - 458s 251ms/step - loss: 7.2073 - recall_at_100: 0.5858 - mrr_at_100: 0.1777 - ndcg_at_100: 0.2573 - map_at_100: 0.1777 - precision_at_100: 0.0059 - regularization_loss: 0.0000e+00 - val_loss: 8.2976 - val_recall_at_100: 0.4232 - val_mrr_at_100: 0.1205 - val_ndcg_at_100: 0.1788 - val_map_at_100: 0.1205 - val_precision_at_100: 0.0042 - val_regularization_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1799/1799 [==============================] - 454s 249ms/step - loss: 7.1735 - recall_at_100: 0.5983 - mrr_at_100: 0.1829 - ndcg_at_100: 0.2639 - map_at_100: 0.1829 - precision_at_100: 0.0060 - regularization_loss: 0.0000e+00 - val_loss: 8.2829 - val_recall_at_100: 0.4237 - val_mrr_at_100: 0.1226 - val_ndcg_at_100: 0.1807 - val_map_at_100: 0.1226 - val_precision_at_100: 0.0042 - val_regularization_loss: 0.0000e+00\n",
      "CPU times: user 1h 29min 16s, sys: 6min 18s, total: 1h 35min 34s\n",
      "Wall time: 1h 16min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model_mlp.fit(train, validation_data=valid, batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "759980ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(rec_list,target):\n",
    "    mrr = 0\n",
    "    for a,b in zip(rec_list,target):\n",
    "        rank = np.argmax(np.array(a)==b)\n",
    "        if rank == 0 and a[0] == b:\n",
    "            mrr += 1\n",
    "        elif rank != 0:\n",
    "            mrr += (1 / (1 + rank))\n",
    "    return mrr/(target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61c6b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model):\n",
    "    valid = Dataset(\n",
    "    [DATA_PROCESSED_FOLDER+'/valid/*.parquet'], \n",
    "    shuffle=False)\n",
    "    x = BatchedDataset(\n",
    "        valid, \n",
    "        batch_size=512, \n",
    "        shuffle=False, \n",
    "    )\n",
    "    predictions = model.predict(x)\n",
    "\n",
    "    topk_predicted = []\n",
    "    for i in range(predictions.shape[0]):\n",
    "        _, topk_indices = tf.math.top_k(predictions[i, :], 100)\n",
    "        topk_predicted.append(topk_indices.numpy().reshape(1, 100))\n",
    "\n",
    "    top_predicted = np.concatenate(topk_predicted)\n",
    "\n",
    "    valid_data = valid.to_ddf().compute().to_pandas()\n",
    "    valid_data['session_id'] = valid_data.session_id.map(session_map)\n",
    "    valid_data = pd.merge(valid_data, purchases, on='session_id')[['session_id', 'item_id']]\n",
    "\n",
    "    valid_data['top100_predicted'] = top_predicted.tolist()\n",
    "    valid_data['top100_predicted']= valid_data['top100_predicted'].apply(lambda x: [item_map[i] for i in x])\n",
    "\n",
    "    return compute_mrr(valid_data['top100_predicted'], valid_data['item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bdca2",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc366604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1292: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 12.4 s, total: 1min 55s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.010065922109270606"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_evaluation(model_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5af8b",
   "metadata": {},
   "source": [
    "## Training Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "721a4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "BI_LSTM_HIDDEN_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90342b6f",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "BiLSTM Block: It requires a dictionary input with the sequence of interaction embeddings `input_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9b7a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(mm.Block):\n",
    "    def __init__(self, hidden_dim= 64, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        lstm = tf.keras.layers.LSTM(hidden_dim, return_sequences=False, dropout=0.05,\n",
    "                                   kernel_regularizer=regularizers.l2(1e-4))\n",
    "        self.lstm = tf.keras.layers.Bidirectional(lstm)\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs, training=False, **kwargs) -> tf.Tensor:  \n",
    "        interactions = inputs['input_sequence']\n",
    "        sequence_representation = self.lstm(interactions)\n",
    "        return sequence_representation\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape = input_shape['input_sequence']\n",
    "        return (input_shape[0], input_shape[1], self.hidden_dim*2)\n",
    "    \n",
    "    \n",
    "bilstm = BiLSTM(hidden_dim=BI_LSTM_HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba2824",
   "metadata": {},
   "source": [
    "InputBlock which takes sequential features, concatenate them and return the sequence of interaction embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a27a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = InputBlock(\n",
    "        schema_model,\n",
    "        aggregation='concat',\n",
    "        seq=True,\n",
    "        max_seq_length=20,\n",
    "        embedding_options=mm.EmbeddingOptions(\n",
    "            embedding_dim_default=128,\n",
    "            infer_embedding_sizes=True,\n",
    "            infer_embedding_sizes_multiplier=2,\n",
    "            infer_embeddings_ensure_dim_multiple_of_8=True\n",
    "        ),\n",
    "        split_sparse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de465608",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block = mm.ParallelBlock({'input_sequence': inputs}).connect(bilstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4f475",
   "metadata": {},
   "source": [
    "MLPBlock to get the sequence of hidden representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f27b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "                [64, 32],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "                dropout=0.01,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7122207",
   "metadata": {},
   "source": [
    "Multi-Classiffication Prediction head which has\n",
    "- Layer Normalization\n",
    "- Weight Tying\n",
    "- Labels as One-hot encoded vectors, used for label smoothing \n",
    "- Temperature Scaling to reduce the overconfidence of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "015330b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_call = L2Norm().connect(\n",
    "    ItemsPredictionWeightTying(schema_model), \n",
    "    mm.LabelToOneHot(), \n",
    "    LogitsTemperatureScaler(temperature=2)\n",
    ")\n",
    "\n",
    "prediction_task = mm.MultiClassClassificationTask(\n",
    "    target_name=\"purchase_id_first\",\n",
    "    pre=prediction_call,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a57bb",
   "metadata": {},
   "source": [
    "Now, we connect all the blocks togther to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "853844eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi_lstm = Model(dense_block, mlp_block, prediction_task)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=3e-1,\n",
    "    clipnorm=True\n",
    ")\n",
    "\n",
    "model_bi_lstm.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=True,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.2),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e064c67",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c4b2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1799/1799 [==============================] - 573s 312ms/step - loss: 13.4683 - recall_at_100: 0.1395 - mrr_at_100: 0.0177 - ndcg_at_100: 0.0389 - map_at_100: 0.0177 - precision_at_100: 0.0014 - regularization_loss: 4.5007 - val_loss: 14.7090 - val_recall_at_100: 0.1800 - val_mrr_at_100: 0.0201 - val_ndcg_at_100: 0.0483 - val_map_at_100: 0.0201 - val_precision_at_100: 0.0018 - val_regularization_loss: 5.5888\n",
      "Epoch 2/10\n",
      "1799/1799 [==============================] - 549s 302ms/step - loss: 14.9613 - recall_at_100: 0.1342 - mrr_at_100: 0.0173 - ndcg_at_100: 0.0377 - map_at_100: 0.0173 - precision_at_100: 0.0013 - regularization_loss: 5.9665 - val_loss: 14.6463 - val_recall_at_100: 0.1752 - val_mrr_at_100: 0.0184 - val_ndcg_at_100: 0.0463 - val_map_at_100: 0.0184 - val_precision_at_100: 0.0018 - val_regularization_loss: 5.5241\n",
      "Epoch 3/10\n",
      "1799/1799 [==============================] - 563s 310ms/step - loss: 20.7259 - recall_at_100: 0.1343 - mrr_at_100: 0.0173 - ndcg_at_100: 0.0378 - map_at_100: 0.0173 - precision_at_100: 0.0013 - regularization_loss: 11.7215 - val_loss: 23.2163 - val_recall_at_100: 0.1839 - val_mrr_at_100: 0.0200 - val_ndcg_at_100: 0.0485 - val_map_at_100: 0.0200 - val_precision_at_100: 0.0018 - val_regularization_loss: 14.0966\n",
      "Epoch 4/10\n",
      "1799/1799 [==============================] - 552s 304ms/step - loss: 24.2127 - recall_at_100: 0.1350 - mrr_at_100: 0.0174 - ndcg_at_100: 0.0380 - map_at_100: 0.0174 - precision_at_100: 0.0013 - regularization_loss: 15.2128 - val_loss: 22.7597 - val_recall_at_100: 0.1780 - val_mrr_at_100: 0.0207 - val_ndcg_at_100: 0.0483 - val_map_at_100: 0.0207 - val_precision_at_100: 0.0018 - val_regularization_loss: 13.6546\n",
      "Epoch 5/10\n",
      "1799/1799 [==============================] - 550s 303ms/step - loss: 23.0834 - recall_at_100: 0.1362 - mrr_at_100: 0.0179 - ndcg_at_100: 0.0386 - map_at_100: 0.0179 - precision_at_100: 0.0014 - regularization_loss: 14.0950 - val_loss: 21.9130 - val_recall_at_100: 0.1764 - val_mrr_at_100: 0.0192 - val_ndcg_at_100: 0.0470 - val_map_at_100: 0.0192 - val_precision_at_100: 0.0018 - val_regularization_loss: 12.8045\n",
      "Epoch 6/10\n",
      "1799/1799 [==============================] - 556s 306ms/step - loss: 22.5991 - recall_at_100: 0.1355 - mrr_at_100: 0.0175 - ndcg_at_100: 0.0381 - map_at_100: 0.0175 - precision_at_100: 0.0014 - regularization_loss: 13.6176 - val_loss: 23.5357 - val_recall_at_100: 0.1743 - val_mrr_at_100: 0.0204 - val_ndcg_at_100: 0.0476 - val_map_at_100: 0.0204 - val_precision_at_100: 0.0017 - val_regularization_loss: 14.4207\n",
      "Epoch 7/10\n",
      "1799/1799 [==============================] - 547s 301ms/step - loss: 22.7144 - recall_at_100: 0.1373 - mrr_at_100: 0.0177 - ndcg_at_100: 0.0386 - map_at_100: 0.0177 - precision_at_100: 0.0014 - regularization_loss: 13.7335 - val_loss: 22.6372 - val_recall_at_100: 0.1807 - val_mrr_at_100: 0.0210 - val_ndcg_at_100: 0.0492 - val_map_at_100: 0.0210 - val_precision_at_100: 0.0018 - val_regularization_loss: 13.5506\n",
      "Epoch 8/10\n",
      "1799/1799 [==============================] - 559s 308ms/step - loss: 22.5754 - recall_at_100: 0.1363 - mrr_at_100: 0.0177 - ndcg_at_100: 0.0384 - map_at_100: 0.0177 - precision_at_100: 0.0014 - regularization_loss: 13.5941 - val_loss: 23.0293 - val_recall_at_100: 0.1804 - val_mrr_at_100: 0.0210 - val_ndcg_at_100: 0.0491 - val_map_at_100: 0.0210 - val_precision_at_100: 0.0018 - val_regularization_loss: 13.9295\n",
      "Epoch 9/10\n",
      "1799/1799 [==============================] - 547s 301ms/step - loss: 22.1267 - recall_at_100: 0.1361 - mrr_at_100: 0.0177 - ndcg_at_100: 0.0384 - map_at_100: 0.0177 - precision_at_100: 0.0014 - regularization_loss: 13.1443 - val_loss: 22.3713 - val_recall_at_100: 0.1887 - val_mrr_at_100: 0.0190 - val_ndcg_at_100: 0.0489 - val_map_at_100: 0.0190 - val_precision_at_100: 0.0019 - val_regularization_loss: 13.2821\n",
      "Epoch 10/10\n",
      "1799/1799 [==============================] - 547s 301ms/step - loss: 22.1145 - recall_at_100: 0.1355 - mrr_at_100: 0.0175 - ndcg_at_100: 0.0382 - map_at_100: 0.0175 - precision_at_100: 0.0014 - regularization_loss: 13.1321 - val_loss: 22.3281 - val_recall_at_100: 0.1816 - val_mrr_at_100: 0.0201 - val_ndcg_at_100: 0.0484 - val_map_at_100: 0.0201 - val_precision_at_100: 0.0018 - val_regularization_loss: 13.2279\n",
      "CPU times: user 1h 46min 28s, sys: 7min 9s, total: 1h 53min 38s\n",
      "Wall time: 1h 32min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model_bi_lstm.fit(train, validation_data=valid, batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5439090",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "268cff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1292: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 11.6 s, total: 1min 58s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.010840330784918362"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_evaluation(model_bi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99784106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf63b42ee7281f212c620f1d25b2d8e8d34e0a013403ff3a06acffed6a925ac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
